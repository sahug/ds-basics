{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boosting Machines Algorithm\n",
    "Gradient boosting refers to a class of ensemble machine learning algorithms that can be used for classification or regression predictive modeling problems.\n",
    "\n",
    "Gradient boosting is also known as gradient tree boosting, stochastic gradient boosting (an extension), and gradient boosting machines, or GBM for short.\n",
    "\n",
    "Ensembles are constructed from decision tree models. Trees are added one at a time to the ensemble and fit to correct the prediction errors made by prior models. This is a type of ensemble machine learning model referred to as boosting.\n",
    "\n",
    "Models are fit using any arbitrary differentiable loss function and gradient descent optimization algorithm. This gives the technique its name, \u201cgradient boosting,\u201d as the loss gradient is minimized as the model is fit, much like a neural network.\n",
    "\n",
    "One way to produce a weighted combination of classifiers which optimizes [the cost] is by gradient descent in function space\n",
    "\n",
    "\u2014 Boosting Algorithms as Gradient Descent in Function Space, 1999.\n",
    "\n",
    "Naive gradient boosting is a greedy algorithm and can overfit the training dataset quickly.\n",
    "\n",
    "It can benefit from regularization methods that penalize various parts of the algorithm and generally improve the performance of the algorithm by reducing overfitting.\n",
    "\n",
    "There are three types of enhancements to basic gradient boosting that can improve performance:\n",
    "\n",
    " - Tree Constraints: such as the depth of the trees and the number of trees used in the ensemble.\n",
    " - Weighted Updates: such as a learning rate used to limit how much each tree contributes to the ensemble.\n",
    " - Random sampling: such as fitting trees on random subsets of features and samples.\n",
    " \n",
    "The use of random sampling often leads to a change in the name of the algorithm to \u201cstochastic gradient boosting.\u201d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boosting Scikit-Learn API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "\n",
    "print(sklearn.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boosting for Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test classification dataset\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "# define dataset\n",
    "X, y = make_classification(\n",
    "    n_samples=1000, n_features=20, n_informative=15, n_redundant=5, random_state=7\n",
    ")\n",
    "# summarize the dataset\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate gradient boosting algorithm for classification\n",
    "from numpy import mean, std\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold, cross_val_score\n",
    "\n",
    "# define dataset\n",
    "X, y = make_classification(\n",
    "    n_samples=1000, n_features=20, n_informative=15, n_redundant=5, random_state=7\n",
    ")\n",
    "\n",
    "# define the model\n",
    "model = GradientBoostingClassifier()\n",
    "\n",
    "# define the evaluation method\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "\n",
    "# evaluate the model on the dataset\n",
    "n_scores = cross_val_score(model, X, y, scoring=\"accuracy\", cv=cv, n_jobs=-1)\n",
    "\n",
    "# report performance\n",
    "print(\"Mean Accuracy: %.3f (%.3f)\" % (mean(n_scores), std(n_scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make predictions using gradient boosting for classification\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "# define dataset\n",
    "X, y = make_classification(\n",
    "    n_samples=1000, n_features=20, n_informative=15, n_redundant=5, random_state=7\n",
    ")\n",
    "\n",
    "# define the model\n",
    "model = GradientBoostingClassifier()\n",
    "\n",
    "# fit the model on the whole dataset\n",
    "model.fit(X, y)\n",
    "\n",
    "# make a single prediction\n",
    "row = [\n",
    "    0.2929949,\n",
    "    -4.21223056,\n",
    "    -1.288332,\n",
    "    -2.17849815,\n",
    "    -0.64527665,\n",
    "    2.58097719,\n",
    "    0.28422388,\n",
    "    -7.1827928,\n",
    "    -1.91211104,\n",
    "    2.73729512,\n",
    "    0.81395695,\n",
    "    3.96973717,\n",
    "    -2.66939799,\n",
    "    3.34692332,\n",
    "    4.19791821,\n",
    "    0.99990998,\n",
    "    -0.30201875,\n",
    "    -4.43170633,\n",
    "    -2.82646737,\n",
    "    0.44916808,\n",
    "]\n",
    "yhat = model.predict([row])\n",
    "\n",
    "# summarize prediction\n",
    "print(\"Predicted Class: %d\" % yhat[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boosting for Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test regression dataset\n",
    "from sklearn.datasets import make_regression\n",
    "\n",
    "# define dataset\n",
    "X, y = make_regression(\n",
    "    n_samples=1000, n_features=20, n_informative=15, noise=0.1, random_state=7\n",
    ")\n",
    "\n",
    "# summarize the dataset\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate gradient boosting ensemble for regression\n",
    "from numpy import mean, std\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.model_selection import RepeatedKFold, cross_val_score\n",
    "\n",
    "# define dataset\n",
    "X, y = make_regression(\n",
    "    n_samples=1000, n_features=20, n_informative=15, noise=0.1, random_state=7\n",
    ")\n",
    "\n",
    "# define the model\n",
    "model = GradientBoostingRegressor()\n",
    "\n",
    "# define the evaluation procedure\n",
    "cv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "\n",
    "# evaluate the model\n",
    "n_scores = cross_val_score(\n",
    "    model, X, y, scoring=\"neg_mean_absolute_error\", cv=cv, n_jobs=-1\n",
    ")\n",
    "\n",
    "# report performance\n",
    "print(\"MAE: %.3f (%.3f)\" % (mean(n_scores), std(n_scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gradient boosting ensemble for making predictions for regression\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "# define dataset\n",
    "X, y = make_regression(\n",
    "    n_samples=1000, n_features=20, n_informative=15, noise=0.1, random_state=7\n",
    ")\n",
    "\n",
    "# define the model\n",
    "model = GradientBoostingRegressor()\n",
    "\n",
    "# fit the model on the whole dataset\n",
    "model.fit(X, y)\n",
    "\n",
    "# make a single prediction\n",
    "row = [\n",
    "    0.20543991,\n",
    "    -0.97049844,\n",
    "    -0.81403429,\n",
    "    -0.23842689,\n",
    "    -0.60704084,\n",
    "    -0.48541492,\n",
    "    0.53113006,\n",
    "    2.01834338,\n",
    "    -0.90745243,\n",
    "    -1.85859731,\n",
    "    -1.02334791,\n",
    "    -0.6877744,\n",
    "    0.60984819,\n",
    "    -0.70630121,\n",
    "    -1.29161497,\n",
    "    1.32385441,\n",
    "    1.42150747,\n",
    "    1.26567231,\n",
    "    2.56569098,\n",
    "    -0.11154792,\n",
    "]\n",
    "yhat = model.predict([row])\n",
    "\n",
    "# summarize prediction\n",
    "print(\"Prediction: %d\" % yhat[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boosting Hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explore Number of Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# explore gradient boosting number of trees effect on performance\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from matplotlib import pyplot\n",
    "% matplotlib inline\n",
    "\n",
    "# get the dataset\n",
    "def get_dataset():\n",
    "    X, y = make_classification(n_samples=1000, n_features=20, n_informative=15, n_redundant=5, random_state=7)\n",
    "    return X, y\n",
    "\n",
    "# get a list of models to evaluate\n",
    "def get_models():\n",
    "    models = dict()\n",
    "    # define number of trees to consider\n",
    "    n_trees = [10, 50, 100, 500, 1000, 5000]\n",
    "    for n in n_trees:\n",
    "        models[str(n)] = GradientBoostingClassifier(n_estimators=n)\n",
    "    return models\n",
    "\n",
    "# evaluate a given model using cross-validation\n",
    "def evaluate_model(model, X, y):\n",
    "    # define the evaluation procedure\n",
    "    cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "    # evaluate the model and collect the results\n",
    "    scores = cross_val_score(model, X, y, scoring='accuracy', cv=cv, n_jobs=-1)\n",
    "    return scores\n",
    "\n",
    "# define dataset\n",
    "X, y = get_dataset()\n",
    "\n",
    "# get the models to evaluate\n",
    "models = get_models()\n",
    "\n",
    "# evaluate the models and store results\n",
    "results, names = list(), list()\n",
    "for name, model in models.items():\n",
    "    # evaluate the model\n",
    "    scores = evaluate_model(model, X, y)\n",
    "    # store the results\n",
    "    results.append(scores)\n",
    "    names.append(name)\n",
    "    # summarize the performance along the way\n",
    "    print('>%s %.3f (%.3f)' % (name, mean(scores), std(scores)))\n",
    "\n",
    "# plot model performance for comparison\n",
    "pyplot.boxplot(results, labels=names, showmeans=True)\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explore Number of Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# explore gradient boosting ensemble number of samples effect on performance\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from numpy import arange\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from matplotlib import pyplot\n",
    "% matplotlib inline\n",
    "\n",
    "# get the dataset\n",
    "def get_dataset():\n",
    "    X, y = make_classification(n_samples=1000, n_features=20, n_informative=15, n_redundant=5, random_state=7)\n",
    "    return X, y\n",
    "\n",
    "# get a list of models to evaluate\n",
    "def get_models():\n",
    "    models = dict()\n",
    "    \n",
    "    # explore sample ratio from 10% to 100% in 10% increments\n",
    "    for i in arange(0.1, 1.1, 0.1):\n",
    "        key = '%.1f' % i\n",
    "        models[key] = GradientBoostingClassifier(subsample=i)\n",
    "        \n",
    "    return models\n",
    "\n",
    "# evaluate a given model using cross-validation\n",
    "def evaluate_model(model, X, y):\n",
    "    \n",
    "    # define the evaluation procedure\n",
    "    cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "    \n",
    "    # evaluate the model and collect the results\n",
    "    scores = cross_val_score(model, X, y, scoring='accuracy', cv=cv, n_jobs=-1)\n",
    "    \n",
    "    return scores\n",
    "\n",
    "# define dataset\n",
    "X, y = get_dataset()\n",
    "\n",
    "# get the models to evaluate\n",
    "models = get_models()\n",
    "\n",
    "# evaluate the models and store results\n",
    "results, names = list(), list()\n",
    "for name, model in models.items():\n",
    "    \n",
    "    # evaluate the model\n",
    "    scores = evaluate_model(model, X, y)\n",
    "    \n",
    "    # store the results\n",
    "    results.append(scores)\n",
    "    names.append(name)\n",
    "    \n",
    "    # summarize the performance along the way\n",
    "    print('>%s %.3f (%.3f)' % (name, mean(scores), std(scores)))\n",
    "    \n",
    "# plot model performance for comparison\n",
    "pyplot.boxplot(results, labels=names, showmeans=True)\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explore Number of Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# explore gradient boosting number of features on performance\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from matplotlib import pyplot\n",
    "% matplotlib inline\n",
    "\n",
    "# get the dataset\n",
    "def get_dataset():\n",
    "    X, y = make_classification(n_samples=1000, n_features=20, n_informative=15, n_redundant=5, random_state=7)\n",
    "    return X, y\n",
    "\n",
    "# get a list of models to evaluate\n",
    "def get_models():\n",
    "    models = dict()\n",
    "    \n",
    "    # explore number of features from 1 to 20\n",
    "    for i in range(1,21):\n",
    "        models[str(i)] = GradientBoostingClassifier(max_features=i)\n",
    "    return models\n",
    "\n",
    "# evaluate a given model using cross-validation\n",
    "def evaluate_model(model, X, y):\n",
    "    \n",
    "    # define the evaluation procedure\n",
    "    cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "    \n",
    "    # evaluate the model and collect the results\n",
    "    scores = cross_val_score(model, X, y, scoring='accuracy', cv=cv, n_jobs=-1)\n",
    "    return scores\n",
    "\n",
    "# define dataset\n",
    "X, y = get_dataset()\n",
    "\n",
    "# get the models to evaluate\n",
    "models = get_models()\n",
    "\n",
    "# evaluate the models and store results\n",
    "results, names = list(), list()\n",
    "\n",
    "for name, model in models.items():\n",
    "    \n",
    "    # evaluate the model\n",
    "    scores = evaluate_model(model, X, y)\n",
    "    \n",
    "    # store the results\n",
    "    results.append(scores)\n",
    "    names.append(name)\n",
    "    \n",
    "    # summarize the performance along the way\n",
    "    print('>%s %.3f (%.3f)' % (name, mean(scores), std(scores)))\n",
    "    \n",
    "# plot model performance for comparison\n",
    "pyplot.boxplot(results, labels=names, showmeans=True)\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explore Learning Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# explore gradient boosting ensemble learning rate effect on performance\n",
    "from matplotlib import pyplot\n",
    "from numpy import mean, std\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold, cross_val_score\n",
    "\n",
    "\n",
    "# get the dataset\n",
    "def get_dataset():\n",
    "    X, y = make_classification(\n",
    "        n_samples=1000, n_features=20, n_informative=15, n_redundant=5, random_state=7\n",
    "    )\n",
    "    return X, y\n",
    "\n",
    "\n",
    "# get a list of models to evaluate\n",
    "def get_models():\n",
    "    models = dict()\n",
    "\n",
    "    # define learning rates to explore\n",
    "    for i in [0.0001, 0.001, 0.01, 0.1, 1.0]:\n",
    "        key = \"%.4f\" % i\n",
    "        models[key] = GradientBoostingClassifier(learning_rate=i)\n",
    "\n",
    "    return models\n",
    "\n",
    "\n",
    "# evaluate a given model using cross-validation\n",
    "def evaluate_model(model, X, y):\n",
    "\n",
    "    # define the evaluation procedure\n",
    "    cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "\n",
    "    # evaluate the model and collect the results\n",
    "    scores = cross_val_score(model, X, y, scoring=\"accuracy\", cv=cv, n_jobs=-1)\n",
    "    return scores\n",
    "\n",
    "\n",
    "# define dataset\n",
    "X, y = get_dataset()\n",
    "\n",
    "# get the models to evaluate\n",
    "models = get_models()\n",
    "\n",
    "# evaluate the models and store results\n",
    "results, names = list(), list()\n",
    "for name, model in models.items():\n",
    "\n",
    "    # evaluate the model\n",
    "    scores = evaluate_model(model, X, y)\n",
    "\n",
    "    # store the results\n",
    "    results.append(scores)\n",
    "    names.append(name)\n",
    "\n",
    "    # summarize the performance along the way\n",
    "    print(\">%s %.3f (%.3f)\" % (name, mean(scores), std(scores)))\n",
    "\n",
    "# plot model performance for comparison\n",
    "pyplot.boxplot(results, labels=names, showmeans=True)\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explore Tree Depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# explore gradient boosting tree depth effect on performance\n",
    "from matplotlib import pyplot\n",
    "from numpy import mean, std\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold, cross_val_score\n",
    "\n",
    "\n",
    "# get the dataset\n",
    "def get_dataset():\n",
    "    X, y = make_classification(\n",
    "        n_samples=1000, n_features=20, n_informative=15, n_redundant=5, random_state=7\n",
    "    )\n",
    "    return X, y\n",
    "\n",
    "\n",
    "# get a list of models to evaluate\n",
    "def get_models():\n",
    "    models = dict()\n",
    "\n",
    "    # define max tree depths to explore between 1 and 10\n",
    "    for i in range(1, 11):\n",
    "        models[str(i)] = GradientBoostingClassifier(max_depth=i)\n",
    "    return models\n",
    "\n",
    "\n",
    "# evaluate a given model using cross-validation\n",
    "def evaluate_model(model, X, y):\n",
    "\n",
    "    # define the evaluation procedure\n",
    "    cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "\n",
    "    # evaluate the model and collect the results\n",
    "    scores = cross_val_score(model, X, y, scoring=\"accuracy\", cv=cv, n_jobs=-1)\n",
    "    return scores\n",
    "\n",
    "\n",
    "# define dataset\n",
    "X, y = get_dataset()\n",
    "\n",
    "# get the models to evaluate\n",
    "models = get_models()\n",
    "\n",
    "# evaluate the models and store results\n",
    "results, names = list(), list()\n",
    "for name, model in models.items():\n",
    "\n",
    "    # evaluate the model\n",
    "    scores = evaluate_model(model, X, y)\n",
    "\n",
    "    # store the results\n",
    "    results.append(scores)\n",
    "    names.append(name)\n",
    "\n",
    "    # summarize the performance along the way\n",
    "    print(\">%s %.3f (%.3f)\" % (name, mean(scores), std(scores)))\n",
    "\n",
    "# plot model performance for comparison\n",
    "pyplot.boxplot(results, labels=names, showmeans=True)\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Grid Search Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example of grid searching key hyperparameters for gradient boosting on a classification dataset\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import GridSearchCV, RepeatedStratifiedKFold\n",
    "\n",
    "# define dataset\n",
    "X, y = make_classification(\n",
    "    n_samples=1000, n_features=20, n_informative=15, n_redundant=5, random_state=7\n",
    ")\n",
    "\n",
    "# define the model with default hyperparameters\n",
    "model = GradientBoostingClassifier()\n",
    "\n",
    "# define the grid of values to search\n",
    "grid = dict()\n",
    "grid[\"n_estimators\"] = [10, 50, 100, 500]\n",
    "grid[\"learning_rate\"] = [0.0001, 0.001, 0.01, 0.1, 1.0]\n",
    "grid[\"subsample\"] = [0.5, 0.7, 1.0]\n",
    "grid[\"max_depth\"] = [3, 7, 9]\n",
    "\n",
    "# define the evaluation procedure\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "\n",
    "# define the grid search procedure\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=model, param_grid=grid, n_jobs=-1, cv=cv, scoring=\"accuracy\"\n",
    ")\n",
    "\n",
    "# execute the grid search\n",
    "grid_result = grid_search.fit(X, y)\n",
    "\n",
    "# summarize the best score and configuration\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "\n",
    "# summarize all scores that were evaluated\n",
    "means = grid_result.cv_results_[\"mean_test_score\"]\n",
    "stds = grid_result.cv_results_[\"std_test_score\"]\n",
    "params = grid_result.cv_results_[\"params\"]\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
