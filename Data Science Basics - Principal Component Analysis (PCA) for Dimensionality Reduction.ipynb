{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After completing this tutorial, you will know:\n",
    "\n",
    "- Dimensionality reduction involves reducing the number of input variables or columns in modeling data.\n",
    "- PCA is a technique from linear algebra that can be used to automatically perform dimensionality reduction.\n",
    "- How to evaluate predictive models that use a PCA projection as input and make predictions with new raw data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tutorial Overview\n",
    "This tutorial is divided into three parts; they are:\n",
    "\n",
    " - Dimensionality Reduction and PCA\n",
    " - PCA Scikit-Learn API\n",
    " - Worked Example of PCA for Dimensionality Reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test classification dataset\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "# define dataset\n",
    "X, y = make_classification(\n",
    "    n_samples=1000, n_features=20, n_informative=15, n_redundant=5, random_state=7\n",
    ")\n",
    "\n",
    "# summarize the dataset\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate pca with logistic regression algorithm for classification\n",
    "from numpy import mean, std\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold, cross_val_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# define dataset\n",
    "X, y = make_classification(\n",
    "    n_samples=1000, n_features=20, n_informative=15, n_redundant=5, random_state=7\n",
    ")\n",
    "\n",
    "# define the pipeline\n",
    "steps = [(\"pca\", PCA(n_components=10)), (\"m\", LogisticRegression())]\n",
    "model = Pipeline(steps=steps)\n",
    "\n",
    "# evaluate model\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "n_scores = cross_val_score(\n",
    "    model, X, y, scoring=\"accuracy\", cv=cv, n_jobs=-1, error_score=\"raise\"\n",
    ")\n",
    "\n",
    "# report performance\n",
    "print(\"Accuracy: %.3f (%.3f)\" % (mean(n_scores), std(n_scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A better approach is to evaluate the same transform and model with different numbers of input features and choose the number of features (amount of dimensionality reduction) that results in the best average performance.\n",
    "\n",
    "The example below performs this experiment and summarizes the mean classification accuracy for each configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare pca number of components with logistic regression algorithm for classification\n",
    "from matplotlib import pyplot\n",
    "from numpy import mean, std\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold, cross_val_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "\n",
    "# get the dataset\n",
    "def get_dataset():\n",
    "    X, y = make_classification(\n",
    "        n_samples=1000, n_features=20, n_informative=15, n_redundant=5, random_state=7\n",
    "    )\n",
    "    return X, y\n",
    "\n",
    "\n",
    "# get a list of models to evaluate\n",
    "def get_models():\n",
    "    models = dict()\n",
    "    for i in range(1, 21):\n",
    "        steps = [(\"pca\", PCA(n_components=i)), (\"m\", LogisticRegression())]\n",
    "        models[str(i)] = Pipeline(steps=steps)\n",
    "    return models\n",
    "\n",
    "\n",
    "# evaluate a given model using cross-validation\n",
    "def evaluate_model(model, X, y):\n",
    "    cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "    scores = cross_val_score(\n",
    "        model, X, y, scoring=\"accuracy\", cv=cv, n_jobs=-1, error_score=\"raise\"\n",
    "    )\n",
    "    return scores\n",
    "\n",
    "\n",
    "# define dataset\n",
    "X, y = get_dataset()\n",
    "\n",
    "# get the models to evaluate\n",
    "models = get_models()\n",
    "\n",
    "# evaluate the models and store results\n",
    "results, names = list(), list()\n",
    "for name, model in models.items():\n",
    "    scores = evaluate_model(model, X, y)\n",
    "    results.append(scores)\n",
    "    names.append(name)\n",
    "    print(\">%s %.3f (%.3f)\" % (name, mean(scores), std(scores)))\n",
    "\n",
    "# plot model performance for comparison\n",
    "pyplot.boxplot(results, labels=names, showmeans=True)\n",
    "pyplot.xticks(rotation=45)\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We may choose to use a PCA transform and logistic regression model combination as our final model.\n",
    "\n",
    "This involves fitting the Pipeline on all available data and using the pipeline to make predictions on new data. Importantly, the same transform must be performed on this new data, which is handled automatically via the Pipeline.\n",
    "\n",
    "The example below provides an example of fitting and using a final model with PCA transforms on new data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make predictions using pca with logistic regression\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# define dataset\n",
    "X, y = make_classification(\n",
    "    n_samples=1000, n_features=20, n_informative=15, n_redundant=5, random_state=7\n",
    ")\n",
    "\n",
    "# define the model\n",
    "steps = [(\"pca\", PCA(n_components=15)), (\"m\", LogisticRegression())]\n",
    "model = Pipeline(steps=steps)\n",
    "\n",
    "# fit the model on the whole dataset\n",
    "model.fit(X, y)\n",
    "\n",
    "# make a single prediction\n",
    "row = [\n",
    "    [\n",
    "        0.2929949,\n",
    "        -4.21223056,\n",
    "        -1.288332,\n",
    "        -2.17849815,\n",
    "        -0.64527665,\n",
    "        2.58097719,\n",
    "        0.28422388,\n",
    "        -7.1827928,\n",
    "        -1.91211104,\n",
    "        2.73729512,\n",
    "        0.81395695,\n",
    "        3.96973717,\n",
    "        -2.66939799,\n",
    "        3.34692332,\n",
    "        4.19791821,\n",
    "        0.99990998,\n",
    "        -0.30201875,\n",
    "        -4.43170633,\n",
    "        -2.82646737,\n",
    "        0.44916808,\n",
    "    ]\n",
    "]\n",
    "yhat = model.predict(row)\n",
    "print(\"Predicted Class: %d\" % yhat[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
